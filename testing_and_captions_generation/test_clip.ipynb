{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP TESTING\n",
    "\n",
    "This file is used to test the vanilla and fine tuned CLIP models under different BLIP configurations. The main usage was to check if the fine tuning was correctly boosting the performance of the base model.\n",
    "\n",
    "Originally just two set of cards were being used for this testing, the one used in the BLIP fine tuning (original set of cards) and the other one not used (found inside this repo: https://github.com/jminuscula/dixit-online/tree/master/cards). After a while, I got access to a new set of cards (Dixit Odissey edition) and I've added also those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\stefa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "blip_model.eval()\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards(path):\n",
    "    images = dict()\n",
    "    for image in os.listdir(path):\n",
    "        raw_image = Image.open(os.path.join(path, image)).convert(\"RGB\")\n",
    "        raw_image = raw_image.resize((224,224))\n",
    "        images[image] = raw_image\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(images, blip_processor, blip_model):\n",
    "    dictionary = dict()\n",
    "    for image_index, raw_image in images.items():\n",
    "        inputs = blip_processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            caption_ids = blip_model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=50,  \n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True, \n",
    "                    top_k=50,       \n",
    "                    top_p=0.95,     \n",
    "                    temperature=0.7,\n",
    "                    repetition_penalty=1.2 \n",
    "                )\n",
    "\n",
    "        caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        dictionary[image_index] = caption\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrects(images, dictionary, clip_processor, clip_model):\n",
    "    corrects = 0\n",
    "    for image_name, caption in dictionary.items():\n",
    "        to_be_compared = [image_name]\n",
    "        \n",
    "        good_sampling = False\n",
    "        while not good_sampling:\n",
    "            to_be_added = random.sample(list(images.keys()), 4)\n",
    "            if not (image_name in to_be_added):\n",
    "                good_sampling = True\n",
    "                to_be_compared.extend(to_be_added)\n",
    "\n",
    "        images_names = to_be_compared\n",
    "        to_be_compared = [images[x] for x in to_be_compared]\n",
    "\n",
    "        inputs = clip_processor(text=caption, images=to_be_compared, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        \n",
    "        logits_per_image = outputs.logits_per_image\n",
    "\n",
    "        probs_per_image = logits_per_image.softmax(dim=0).squeeze()\n",
    "        max_score_idx = torch.argmax(probs_per_image).item()\n",
    "        best_image = images_names[max_score_idx]\n",
    "\n",
    "        if best_image == image_name:\n",
    "            corrects += 1\n",
    "    return corrects / len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_over_cards(path, blip_processor, blip_model, clip_processor, clip_model):\n",
    "    images = get_cards(path)\n",
    "    dictionary = get_captions(images, blip_processor, blip_model)\n",
    "    accuracy = check_corrects(images, dictionary, clip_processor, clip_model)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_accuracies(blip_processor, blip_model, clip_processor, clip_model, path1 = \"../cards/original_cards\", path2 = \"../cards/online_cards\", clip_weights_path = None, blip_weights_path = None, num_iterations = 5):\n",
    "\n",
    "    if clip_weights_path != None:\n",
    "        clip_model.load_state_dict(torch.load(clip_weights_path))\n",
    "\n",
    "    if blip_weights_path != None:\n",
    "        blip_model.load_state_dict(torch.load(blip_weights_path))\n",
    "\n",
    "    tot_first_accuracy = []\n",
    "    tot_second_accuracy = []\n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        first_cards_accuracy = check_accuracy_over_cards(path1, blip_processor, blip_model, clip_processor, clip_model)\n",
    "        second_cards_accuracy = check_accuracy_over_cards(path2, blip_processor, blip_model, clip_processor, clip_model)\n",
    "\n",
    "        tot_first_accuracy.append(first_cards_accuracy)\n",
    "        tot_second_accuracy.append(second_cards_accuracy)\n",
    "\n",
    "    print(f\"First card set: {sum(tot_first_accuracy)/len(tot_first_accuracy)}\")\n",
    "    print(f\"Second card set: {sum(tot_second_accuracy)/len(tot_second_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_accuracies_all(blip_processor, blip_model, clip_processor, clip_model, path1 = \"../cards/original_cards\", path2 = \"../cards/online_cards\", path3=\"../cards/odissey_cards\", clip_weights_path = None, blip_weights_path = None, num_iterations = 5):\n",
    "    if clip_weights_path != None:\n",
    "        clip_model.load_state_dict(torch.load(clip_weights_path))\n",
    "\n",
    "    if blip_weights_path != None:\n",
    "        blip_model.load_state_dict(torch.load(blip_weights_path))\n",
    "\n",
    "    tot_first_accuracy = []\n",
    "    tot_second_accuracy = []\n",
    "    tot_third_accuracy = []\n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        first_cards_accuracy = check_accuracy_over_cards(path1, blip_processor, blip_model, clip_processor, clip_model)\n",
    "        second_cards_accuracy = check_accuracy_over_cards(path2, blip_processor, blip_model, clip_processor, clip_model)\n",
    "        third_cards_accuracy = check_accuracy_over_cards(path3, blip_processor, blip_model, clip_processor, clip_model)\n",
    "\n",
    "        tot_first_accuracy.append(first_cards_accuracy)\n",
    "        tot_second_accuracy.append(second_cards_accuracy)\n",
    "        tot_third_accuracy.append(third_cards_accuracy)\n",
    "\n",
    "    print(f\"First card set: {sum(tot_first_accuracy)/len(tot_first_accuracy)}\")\n",
    "    print(f\"Second card set: {sum(tot_second_accuracy)/len(tot_second_accuracy)}\")\n",
    "    print(f\"Third card set: {sum(tot_third_accuracy)/len(tot_third_accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How testing is performed\n",
    "\n",
    "The models are tested on different card sets for $n$ iterations (five by default). The number printed beside the names of the card sets is the average accuracy over the iterations of the CLIP model. Captions to be used with images for the CLIP model are extracted using the fine tuned (or base) Blip model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Blip model with base CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:38<00:00, 67.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First card set: 0.9428571428571428\n",
      "Second card set: 0.9299999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_accuracies(blip_processor, blip_model, clip_processor, clip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuned Blip model on first set of rephrased captions and base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:19<00:00, 123.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First card set: 0.7904761904761906\n",
      "Second card set: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_accuracies(blip_processor, blip_model, clip_processor, clip_model, blip_weights_path=\"../weights/rephrased_blip/epoch35.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuned Blip model on first set of rephrased captions. Fine tuned CLIP model using fine tuned Blip on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:07<00:00, 121.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First card set: 0.8619047619047621\n",
      "Second card set: 0.7979999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#clip patch16\n",
    "compare_accuracies(blip_processor, blip_model, clip_processor, clip_model, blip_weights_path=\"../weights/rephrased_blip/epoch35.pt\", clip_weights_path=\"../weights/rephrased_coco_clip/epoch14.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuned Blip model on second set of rephrased captions with base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [09:01<00:00, 108.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First card set: 0.45714285714285713\n",
      "Second card set: 0.48200000000000004\n",
      "Third card set: 0.37857142857142856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#clip patch16\n",
    "compare_accuracies_all(blip_processor, blip_model, clip_processor, clip_model, blip_weights_path=\"../weights/rephrased_blip(2nd)/epoch50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuned Blip model on second set of reprhased captions. Fine tuned CLIP model using fine tuned Blip on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [09:12<00:00, 110.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First card set: 0.488095238095238\n",
      "Second card set: 0.512\n",
      "Third card set: 0.41904761904761906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#clip patch16\n",
    "compare_accuracies_all(blip_processor, blip_model, clip_processor, clip_model, blip_weights_path=\"../weights/rephrased_blip(2nd)/epoch50.pt\", clip_weights_path=\"../weights/rephrased_coco_clip(2nd)/epoch13.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
