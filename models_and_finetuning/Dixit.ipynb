{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DIXIT\n",
        "\n",
        "The following notebook contains the experiments performed to obtain a bot that can play a full game of Dixit.\n",
        "\n",
        "The code is organized as follows:\n",
        "\n",
        "*   MOUNT DRIVE - IMPORT LIBRARIES - UNZIP DATA  \n",
        "    This section contains all the code needed to run the experiments. In particular, the drive is mounted, the COCO dataset in unzipped and the libraries are imported (also the seed for randomness are set to ensure reproducibility)\n",
        "\n",
        "*   BLIP CAPTIONS REPHRASING USING ZEPHYR  \n",
        "    In this section the simple descriptive captions extracted with a BLIP model are rephrased using an open-source LLM. The choice for this particular model was motivated by some experiments, showing that it was good enough in performing this particular task. Also, given the amount of captions to be rephrased, an open source model without any limitations was the optimal choice to avoid paying expensive LLM APIs or wait days to perform the task using the free ones.\n",
        "\n",
        "    NOTE 1: AN HUGGING FACE KEY IS REQUIRED TO PERFORM THE TASK!  \n",
        "\n",
        "    NOTE 2: THE REPHRASING CAN BE DONE USING THE FREE GPU AVAILABLE ON COLAB (T4), BUT IT TAKES 12 HOURS!\n",
        "\n",
        "*   BLIP FINE TUNING(ONLINE - REPHRASED)  \n",
        "    This block contains the code used to perform the fine tuning of the BLIP model for both the data found online and the rephrased captions obtained using the Zephyr LLM. Given that there were very few Dixit images, the visual part of the model was frozen during fine tuning. Also, the dataset found online presented a lot of problems, probably because it contained too noisy data, or data from which anything meaningful could be learned.\n",
        "\n",
        "    NOTE: For this task an L4 GPU was used\n",
        "\n",
        "*   CLIP FINE TUNING(ONLINE - REPHRASED)  \n",
        "    The section contains the code used to fine tune CLIP on both the data found online and the ones produced with the rephrasing. The performances where not great, and even with a fine tune of just the projection layers (the ones that project the text and image features in a common embedding space), the model kept overfitting or not learning anything meaningful. Probably the Dixit images where still too few, even with just the projection layers to be fine tuned. Also, the same problems regarding the online data discovered during the BLIP fine tuning were also detected here.\n",
        "\n",
        "*   FINE TUNING CLIP WITH FINE TUNED BLIP ON COCO  \n",
        "    The block contains the code used to fine tune CLIP using COCO dataset. The process was performed in the following way: given an image inside the dataset, the fine tuned BLIP model was used to extract a caption, to then obtain an image with its own creative caption, to be used to fine tune CLIP. The idea was that in this way, the model could learn from more general images without overfitting, to then be used in Dixit. This fine tuning bettered the performance of the model by 4-5%.\n",
        "\n",
        "\n",
        "*Additional info:*  \n",
        "Throughout the whole notebook, among all the experiments that require a number of epochs to be defined, a standard number (100) was chosen, large enough to ensure a long training if needed. Despite this, in every single experiment, an early manual stopping was performed.\n",
        "\n",
        "Also, in order to use the images inside a custom Dataset, a standard image size (224x224) was used for all the models that required images as input (BLIP and CLIP). This format was not chosen for a particular reason except standardization among all the experiments. However, by default, the CLIP processor, before feeding the images to the model, resize them in this exact same format.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3gQGd6RgSLvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOUNT DRIVE - IMPORT LIBRARIES - UNZIP DATA"
      ],
      "metadata": {
        "id": "D6z3783_DYeu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofehHzEL2iD3"
      },
      "source": [
        "### Mount drive for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sBYbltzr6mx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip COCO dataset"
      ],
      "metadata": {
        "id": "eDSxXcqEbVmi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWZ68MDWvB8n"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Dixit/COCO_Dataset/train2014.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zFJ8ZGYwBfC"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Dixit/COCO_Dataset/val2014.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-VR9UlwGr4"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Dixit/COCO_Dataset/annotations_trainval2014.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6SusgF3ucG9"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR24gsnQp8aE"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "random.seed(23)\n",
        "torch.manual_seed(23)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLIP CAPTIONS REPHRASING USING ZEPHYR\n",
        "\n",
        "The following block contains the code used to perform the rephrasing. Given that this process was performed with two different prompts, everything was left as it was during the second prompt rephrasing. The first prompt remains as a comment inside the main block whose purpose is to do the actual rephrasing. The path here written were the ones used to save the second prompt rephrasing.\n"
      ],
      "metadata": {
        "id": "GPXR2_gyi1Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following package is needed to run the LLM"
      ],
      "metadata": {
        "id": "EeHQHDrxeHdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "QrTC3iYjr9yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"\" # <-- Insert your hugging face key\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\", token = token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True, token = token)"
      ],
      "metadata": {
        "id": "y5qFCgUsi4lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the rephrasing takes almost 12 hours, the process can be both started and resumed to the last iteration using the following method"
      ],
      "metadata": {
        "id": "7uYPPq7jeg3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_rephrasing(path_to_captions, path_to_rephrased_captions):\n",
        "    with open(path_to_captions, 'r') as f:\n",
        "        captions_dict = json.load(f)\n",
        "\n",
        "    if not os.path.exists(path_to_rephrased_captions):\n",
        "      with open(path_to_rephrased_captions, 'w') as f:\n",
        "        json.dump({}, f)\n",
        "      return captions_dict\n",
        "\n",
        "    with open(path_to_rephrased_captions, 'r') as f:\n",
        "        rephrased_captions_dict = json.load(f)\n",
        "\n",
        "    return {k:v for k,v in captions_dict.items() if k not in rephrased_captions_dict}"
      ],
      "metadata": {
        "id": "ROxTQ95MeiuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_dict = start_rephrasing(\"/content/drive/MyDrive/Dixit/captions/captions(original_images).json\", \"/content/drive/MyDrive/Dixit/captions/rephrased_captions(2nd_prompt).json\")"
      ],
      "metadata": {
        "id": "IQOoi6vNgJsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BE AWARE: It takes almost 12 hours on a T4 GPU!"
      ],
      "metadata": {
        "id": "ut0SUjgwMPHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for image, captions in tqdm(captions_dict.items(), desc=\"REPHRASING...\"):\n",
        "\n",
        "    rephrased_captions = []\n",
        "\n",
        "    for caption in captions:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                #\"content\": \"You are a chatbot whose purpose is to rephrase captions in the shortest, most creative, mysterious and vague way.\", (1st prompt)\n",
        "                \"content\": \"You are a chatbot whose purpose is to use three words representing emotions and abstract reasoning to summarize a caption.\", #(2nd prompt)\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": caption},\n",
        "        ]\n",
        "\n",
        "        model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        input_length = model_inputs.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          generated_ids = model.generate(\n",
        "              model_inputs,\n",
        "              max_new_tokens=30,\n",
        "              num_return_sequences=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              repetition_penalty=1.2,\n",
        "              no_repeat_ngram_size=3,\n",
        "              eos_token_id=tokenizer.eos_token_id,\n",
        "              do_sample=True\n",
        "          )\n",
        "\n",
        "        rephrased = tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)\n",
        "\n",
        "        rephrased_captions.extend(rephrased)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/Dixit/captions/rephrased_captions(2nd_prompt).json\", 'r') as f:\n",
        "        rephrased_captions_dict = json.load(f)\n",
        "\n",
        "    rephrased_captions_dict[image] = rephrased_captions\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/Dixit/captions/rephrased_captions(2nd_prompt).json\", 'w') as f:\n",
        "      json.dump(rephrased_captions_dict, f)"
      ],
      "metadata": {
        "id": "n_ajsH1IzKYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLIP FINE TUNING(ONLINE - REPHRASED)\n",
        "\n",
        "The following cells contain the code to perform fine tuning on the BLIP model for both the dataset found online and the rephrased captions. Again, given that there are two different dataset for the rephrased captions obtained with different prompts, the paths involving the rephrased dataset here used refer to the fine tuning done with the second prompt dataset"
      ],
      "metadata": {
        "id": "Krm8tvOqNS8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMAGES LOADING"
      ],
      "metadata": {
        "id": "BnqxDkmuE3u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_csv(path):\n",
        "    df = pd.read_csv (path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "GDI54Zb0y4Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Dixit/dixit_cards\"\n",
        "\n",
        "images = dict()\n",
        "for image_name in os.listdir(image_path):\n",
        "  image = Image.open(os.path.join(image_path, image_name)).convert('RGB')\n",
        "  images[int(image_name.split(\".\")[0])] = image"
      ],
      "metadata": {
        "id": "b38Dxeu73Bp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET CREATION"
      ],
      "metadata": {
        "id": "Xq0EVvmyE7z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose which dataset (and so which kind of data) to use for the fine tuning"
      ],
      "metadata": {
        "id": "RTCP9gXmFMDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================================================================================="
      ],
      "metadata": {
        "id": "wJrFK7mPf5O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine tune BLIP using the dixit data found online, run the following cells:"
      ],
      "metadata": {
        "id": "K0iJq9mpfF9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DixitDataset(Dataset):\n",
        "  def __init__(self, images, annotations, processor):\n",
        "    self.images = images\n",
        "    self.annotations = annotations\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    card, caption = self.annotations[index]\n",
        "    image = self.images[card]\n",
        "\n",
        "    inputs = processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "    for k in inputs.keys():\n",
        "      inputs[k] = inputs[k].squeeze()\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "MTdXhB1ZQReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the model was learning to output blank captions if trained on the untouched data found online, an idea was to train just with longer captions to see how it would behave. After different attempts with different lenghts (the last one being lenght = 4, here used in the following cell), it was clear that those data were too noisy and incomplete."
      ],
      "metadata": {
        "id": "L_csBgAgnAOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_path = \"/content/drive/MyDrive/Dixit/dixit.csv\"\n",
        "df = open_csv(annotation_path)\n",
        "list_of_dicts = df.to_dict(orient='records')\n",
        "\n",
        "captions_list = []\n",
        "for d in list_of_dicts:\n",
        "  if len(d[\"DESCRIPTION\"].split()) > 3:\n",
        "    narrator = -1\n",
        "    for i in range(1, d[\"PLAYERS\"] + 1):\n",
        "      if d[f\"C{i}_TARGET\"]:\n",
        "        narrator = i\n",
        "        break\n",
        "\n",
        "    card_number = int(d[f\"C{narrator}_CARD\"])\n",
        "    captions_list.append((card_number, d[\"DESCRIPTION\"]))"
      ],
      "metadata": {
        "id": "R5lAPHrB3PuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================================================================================="
      ],
      "metadata": {
        "id": "1Ei-VaJGf7qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine tune BLIP on rephrased captions, run the following cells:"
      ],
      "metadata": {
        "id": "_oFOQ6Txff43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DixitDataset(Dataset):\n",
        "  def __init__(self, images, annotations, processor):\n",
        "    self.images = images\n",
        "    self.annotations = annotations\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    card, captions = self.annotations[index]\n",
        "    image = self.images[card]\n",
        "\n",
        "    caption = random.choice(captions)\n",
        "\n",
        "    inputs = processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "    for k in inputs.keys():\n",
        "      inputs[k] = inputs[k].squeeze()\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "0KPqPkIZleOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Dixit/captions/rephrased_captions(2nd_prompt).json\"\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "captions_list = []\n",
        "for image, captions in data.items():\n",
        "  captions_list.append((int(image.split(\".\")[0]), captions))"
      ],
      "metadata": {
        "id": "IPheUBBTfjs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HYPERPARAMETERS AND CODE TO DO TRAINING"
      ],
      "metadata": {
        "id": "9jympMBpFDTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.15\n",
        "\n",
        "random.shuffle(captions_list)\n",
        "train_captions = captions_list[int(len(captions_list)*split):]\n",
        "val_captions = captions_list[:int(len(captions_list)*split)]"
      ],
      "metadata": {
        "id": "qV-OEG1v4zef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-5\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "nepochs = 100\n",
        "\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "fX7O9d4BmVXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
      ],
      "metadata": {
        "id": "UlNL2bOt-OlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DixitDataset(images, train_captions, processor)\n",
        "val_dataset = DixitDataset(images, val_captions, processor)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
      ],
      "metadata": {
        "id": "mbe55xg-1-I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the very small amount of images (84), compared to the thousand captions from both the online and rephrased data, the BLIP model was overfitting pretty quickly the cards provided. To avoid this the vision model was frozen, so that just the text part was being fine tuned."
      ],
      "metadata": {
        "id": "ZSzJTuGwo-Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in caption_model.vision_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "LV2c7BccfFTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(caption_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "zsYVA8jS-bWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, caption_model, train_dataloader, val_dataloader, optimizer, path, start_epoch = 0):\n",
        "  for epoch in trange(start_epoch+1, epochs, leave=False, desc=\"Epoch\"):\n",
        "      loss_epoch = 0\n",
        "      caption_model.train()\n",
        "      for inputs in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
        "\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          outputs = caption_model(pixel_values = inputs[\"pixel_values\"], input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"], labels = inputs[\"input_ids\"])\n",
        "          loss = outputs.loss\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_epoch += loss.item()\n",
        "\n",
        "      vloss_epoch = 0\n",
        "      caption_model.eval()\n",
        "      with torch.no_grad():\n",
        "        for inputs in tqdm(val_dataloader, desc=\"Validation\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            outputs = caption_model(pixel_values = inputs[\"pixel_values\"], input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"], labels = inputs[\"input_ids\"])\n",
        "            vloss = outputs.loss\n",
        "            vloss_epoch += vloss.item()\n",
        "\n",
        "      metrics =  f\"EPOCH {epoch}/{epochs}. Training Loss: {loss_epoch/len(train_dataloader)} Validation Loss: {vloss_epoch/len(val_dataloader)}\"\n",
        "      with open(os.path.join(path, \"metrics.txt\"), \"a\") as f:\n",
        "          f.write(metrics + \"\\n\")\n",
        "          torch.save(caption_model.state_dict(), os.path.join(path, \"weights\", f\"epoch{epoch}.pt\"))\n",
        "          torch.save(optimizer.state_dict(), os.path.join(path, \"optimizer\", f\"epoch{epoch}.pt\"))\n",
        "      print(metrics)"
      ],
      "metadata": {
        "id": "NByAgvIW-3Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RESUME TRAINING"
      ],
      "metadata": {
        "id": "byItLeJcEUoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If for some reason Google Colab kick you during the training, run this cell with your values to resume the training"
      ],
      "metadata": {
        "id": "fnsfWpbAEjwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = -1\n",
        "path_to_weights = \"\"\n",
        "path_to_optimizer = \"\"\n",
        "caption_model.load_state_dict(torch.load(path_to_weights))\n",
        "optimizer.load_state_dict(torch.load(path_to_optimizer))"
      ],
      "metadata": {
        "id": "9KzNggRl4lSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAIN"
      ],
      "metadata": {
        "id": "4lYne4RDEajY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An L4 GPU was used for this training"
      ],
      "metadata": {
        "id": "rpD8Eflwva93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save = \"\""
      ],
      "metadata": {
        "id": "u14meELPjj6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(nepochs, caption_model, train_dataloader, val_dataloader, optimizer, path_to_save)"
      ],
      "metadata": {
        "id": "943oGY-eAMyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP FINE TUNING (ONLINE - REPHRASED)\n",
        "\n",
        "Block containing the code used to fine tune CLIP on both the dataset found online and the one created with Zephyr. Again, given that there are two different dataset for the rephrased captions obtained with different prompts, the paths involving the rephrased dataset here used refer to the fine tuning done with the second prompt dataset"
      ],
      "metadata": {
        "id": "Ar1pUYn-k36V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_csv(path):\n",
        "    df = pd.read_csv (path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "R-omB9kFk59N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Dixit/dixit_cards\"\n",
        "\n",
        "images = dict()\n",
        "for image_name in os.listdir(image_path):\n",
        "  image = Image.open(os.path.join(image_path, image_name)).convert('RGB')\n",
        "  images[int(image_name.split(\".\")[0])] = image"
      ],
      "metadata": {
        "id": "xSo2ttePlAzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================================================================="
      ],
      "metadata": {
        "id": "_GmU31u16-TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to fine tune clip on data found online:"
      ],
      "metadata": {
        "id": "m5QYRDYk7E2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DixitDataset(Dataset):\n",
        "  def __init__(self, images, annotations, processor):\n",
        "    self.images = images\n",
        "    self.annotations = annotations\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    card, caption = self.annotations[index]\n",
        "    image = self.images[card]\n",
        "\n",
        "    inputs = self.processor(text=caption, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
        "\n",
        "    return {k:v.squeeze() for k,v in inputs.items()}"
      ],
      "metadata": {
        "id": "yfHQhT7zk-TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_path = \"/content/drive/MyDrive/Dixit/dixit.csv\"\n",
        "df = open_csv(annotation_path)\n",
        "list_of_dicts = df.to_dict(orient='records')\n",
        "\n",
        "captions_list = []\n",
        "for d in list_of_dicts:\n",
        "  if len(d[\"DESCRIPTION\"].split()) > 3:\n",
        "    narrator = -1\n",
        "    for i in range(1, d[\"PLAYERS\"] + 1):\n",
        "      if d[f\"C{i}_TARGET\"]:\n",
        "        narrator = i\n",
        "        break\n",
        "\n",
        "    card_number = int(d[f\"C{narrator}_CARD\"])\n",
        "    captions_list.append((card_number, d[\"DESCRIPTION\"]))"
      ],
      "metadata": {
        "id": "4iYuoRNFlDDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to fine tune clip of rephrased captions:"
      ],
      "metadata": {
        "id": "a_VON9Df7Ku_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DixitDataset(Dataset):\n",
        "  def __init__(self, images, annotations, processor):\n",
        "    self.images = images\n",
        "    self.annotations = annotations\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    card, captions = self.annotations[index]\n",
        "    image = self.images[card]\n",
        "\n",
        "    caption = random.choice(captions)\n",
        "\n",
        "    inputs = self.processor(text=caption, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
        "\n",
        "    return {k:v.squeeze() for k,v in inputs.items()}"
      ],
      "metadata": {
        "id": "yppuwQtwaF5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Dixit/captions/rephrased_captions(2nd_prompt).json\"\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "captions_list = []\n",
        "for image, captions in data.items():\n",
        "  captions_list.append((int(image.split(\".\")[0]), captions))"
      ],
      "metadata": {
        "id": "3eGrxc8E7auZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================================================================="
      ],
      "metadata": {
        "id": "H4Buq-oV7J0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.10\n",
        "\n",
        "random.shuffle(captions_list)\n",
        "train_captions = captions_list[int(len(captions_list)*split):]\n",
        "val_captions = captions_list[:int(len(captions_list)*split)]"
      ],
      "metadata": {
        "id": "vlRyqnlalFBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"openai/clip-vit-base-patch16\"\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-7)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "UlUyr8TyxUig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_dataset = DixitDataset(images, train_captions, processor)\n",
        "val_dataset = DixitDataset(images, val_captions, processor)\n",
        "\n",
        "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "60E_U7DhvVv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the CLIP model was consistently overfitting, to mitigate this behaviour the idea was to fine tune just the projection layers. Those layers project the text and images features in a common embedding space. It is reasonable to think that a pretrained CLIP model can already extract meaningful features, the problem is how to represent them in a common space."
      ],
      "metadata": {
        "id": "wIPJ3_eTX2hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.visual_projection.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.text_projection.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "4eqk2JyAxsB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, model, processor, data_loader_train, data_loader_val, optimizer, path, start_epoch = 0):\n",
        "  for epoch in trange(start_epoch+1, epochs, leave=False, desc=\"Epoch\"):\n",
        "      loss_epoch = 0\n",
        "      model.train()\n",
        "      for inputs in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
        "\n",
        "          loss = model(**inputs, return_loss = True).loss\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_epoch += loss.item()\n",
        "\n",
        "      vloss_epoch = 0\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for inputs in tqdm(data_loader_val, desc=\"Validation\", leave=False):\n",
        "\n",
        "            vloss = model(**inputs, return_loss = True).loss\n",
        "\n",
        "            vloss_epoch += vloss.item()\n",
        "\n",
        "      metrics =  f\"EPOCH {epoch}/{epochs}. Training Loss: {loss_epoch/len(data_loader_train)} Validation Loss: {vloss_epoch/len(data_loader_val)}\"\n",
        "      with open(os.path.join(path, \"metrics.txt\"), \"a\") as f:\n",
        "          f.write(metrics + \"\\n\")\n",
        "      torch.save(model.state_dict(), os.path.join(path, \"weights\", f\"epoch{epoch}.pt\"))\n",
        "      torch.save(optimizer.state_dict(), os.path.join(path, \"optimizer\", f\"epoch{epoch}.pt\"))\n",
        "      print(metrics)"
      ],
      "metadata": {
        "id": "sjWaxNTuoibe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save = \"\"\n",
        "train(30, model, processor, data_loader_train, data_loader_val, optimizer, path_to_save, start_epoch = 0)"
      ],
      "metadata": {
        "id": "a9tOkMwFu31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINE TUNING CLIP WITH FINE TUNED BLIP ON COCO\n",
        "\n",
        "This block contain the CLIP fine tuning on the COCO Dataset, with captions extracted with the fine tuned BLIP model"
      ],
      "metadata": {
        "id": "zAmPVbZI4S7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set ='/content/train2014'\n",
        "val_set = '/content/val2014'\n",
        "\n",
        "train_images = [os.path.join(train_set, x) for x in os.listdir(train_set) if x.endswith('.jpg')]\n",
        "\n",
        "val_images = [os.path.join(val_set, x) for x in os.listdir(val_set) if x.endswith('.jpg')]"
      ],
      "metadata": {
        "id": "3kwNa2kA6KR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with only 10% of original images, or else an epoch can take some hours\n",
        "percentage = 0.1\n",
        "\n",
        "train_images = train_images[:int(len(train_images)*percentage)]\n",
        "val_images = val_images[:int(len(val_images)*percentage)]"
      ],
      "metadata": {
        "id": "hzfSKfKY6sEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "UEbHhA4uI8pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "blip_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Dixit/training_results/BLIP-rephrased(2nd_prompt)/weights/epoch50.pt\"))\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "JA1SVqqNKGuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained it the \"CLIP FINE TUNING (ONLINE - REPHRASED)\" section, just the projection layers are fine tuned to avoid overfitting"
      ],
      "metadata": {
        "id": "8WIh6VU6hcx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in clip_model.visual_projection.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in clip_model.text_projection.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "B3sOVybIJtYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train CLIP on COCO creative captions"
      ],
      "metadata": {
        "id": "gHYiQHdJHWs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class COCORephrasedDataset(Dataset):\n",
        "  def __init__(self, path_images, transform = None):\n",
        "    self.path_images = path_images\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.path_images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    path_image = self.path_images[index]\n",
        "\n",
        "    image = Image.open(path_image).convert(\"RGB\")\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image"
      ],
      "metadata": {
        "id": "5HF2k4gJHaci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the BLIP processor already perform a rescaling on the pixel values, the usual transform.ToTensor() is a pain, because it does the same rescaling. If done twice, the model predict completely wrong captions. To avoid this, a custom transform is created. It does exactly the same as ToTensor() but without any rescaling."
      ],
      "metadata": {
        "id": "S9X2Q4n6bPBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((image_size, image_size)), transforms.Lambda(lambda pic: torch.tensor(np.array(pic), dtype=torch.float32).permute(2, 0, 1))])\n",
        "\n",
        "train_dataset = COCORephrasedDataset(train_images, transform = transform)\n",
        "val_dataset = COCORephrasedDataset(val_images, transform = transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "r1ykntW0H_4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits_per_image, logits_per_text):\n",
        "    image_to_text_preds = logits_per_image.argmax(dim=-1)\n",
        "    image_to_text_acc = (image_to_text_preds == torch.arange(len(image_to_text_preds), device=device)).float().mean()\n",
        "\n",
        "    text_to_image_preds = logits_per_text.argmax(dim=-1)\n",
        "    text_to_image_acc = (text_to_image_preds == torch.arange(len(text_to_image_preds), device=device)).float().mean()\n",
        "\n",
        "    return image_to_text_acc.item(), text_to_image_acc.item()"
      ],
      "metadata": {
        "id": "6W5poYkM_nHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, clip_model, clip_processor, blip_model, blip_processor, train_dataloader, val_dataloader, optimizer, path, start_epoch = 0):\n",
        "  blip_model.eval()\n",
        "  for epoch in trange(start_epoch+1, epochs, leave=False, desc=\"Epoch\"):\n",
        "      loss_epoch = 0\n",
        "\n",
        "      clip_model.train()\n",
        "\n",
        "      tot_image_to_text_acc = 0\n",
        "      tot_text_to_image_acc = 0\n",
        "      tot_training_samples = 0\n",
        "\n",
        "      for inputs in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
        "\n",
        "          to_caption = blip_processor(inputs, return_tensors=\"pt\").to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "            captions = blip_model.generate(\n",
        "                  **to_caption,\n",
        "                  max_length=50,\n",
        "                  num_return_sequences=1,\n",
        "                  do_sample=True,\n",
        "                  top_k=50,\n",
        "                  top_p=0.95,\n",
        "                  temperature=0.7,\n",
        "                  repetition_penalty=1.2,\n",
        "                  no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "          to_clip = [blip_processor.decode(caption, skip_special_tokens=True) for caption in captions]\n",
        "\n",
        "          inputs_clip = clip_processor(text=to_clip, images=inputs, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
        "\n",
        "          outputs = clip_model(**inputs_clip, return_loss=True)\n",
        "\n",
        "          loss = outputs.loss\n",
        "          logits_per_image = outputs.logits_per_image\n",
        "          logits_per_text = outputs.logits_per_text\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_epoch += loss.item()\n",
        "\n",
        "          image_to_text_acc, text_to_image_acc = compute_accuracy(logits_per_image, logits_per_text)\n",
        "\n",
        "          tot_image_to_text_acc += image_to_text_acc\n",
        "          tot_text_to_image_acc += text_to_image_acc\n",
        "          tot_training_samples += 1\n",
        "\n",
        "\n",
        "      vloss_epoch = 0\n",
        "      clip_model.eval()\n",
        "\n",
        "      v_tot_image_to_text_acc = 0\n",
        "      v_tot_text_to_image_acc = 0\n",
        "      tot_validation_samples = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for inputs in tqdm(val_dataloader, desc=\"Validation\", leave=False):\n",
        "\n",
        "            to_caption = blip_processor(inputs, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            captions = blip_model.generate(\n",
        "                  **to_caption,\n",
        "                  max_length=50,\n",
        "                  num_return_sequences=1,\n",
        "                  do_sample=True,\n",
        "                  top_k=50,\n",
        "                  top_p=0.95,\n",
        "                  temperature=0.7,\n",
        "                  repetition_penalty=1.2,\n",
        "                  no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "            to_clip = [blip_processor.decode(caption, skip_special_tokens=True) for caption in captions]\n",
        "\n",
        "            inputs_clip = clip_processor(text=to_clip, images=inputs, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
        "\n",
        "            outputs = clip_model(**inputs_clip, return_loss=True)\n",
        "\n",
        "            vloss = outputs.loss\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "            logits_per_text = outputs.logits_per_text\n",
        "\n",
        "            vloss_epoch += vloss.item()\n",
        "\n",
        "            v_image_to_text_acc, v_text_to_image_acc = compute_accuracy(logits_per_image, logits_per_text)\n",
        "\n",
        "            v_tot_image_to_text_acc += v_image_to_text_acc\n",
        "            v_tot_text_to_image_acc += v_text_to_image_acc\n",
        "            tot_validation_samples += 1\n",
        "\n",
        "      metrics =  f\"EPOCH {epoch}/{epochs}. Training Loss: {loss_epoch/len(train_dataloader)} Validation Loss: {vloss_epoch/len(val_dataloader)}\"\n",
        "      metrics += f\"\\nTraining image to text accuracy: {tot_image_to_text_acc/tot_training_samples} Training text to image accuracy: {tot_text_to_image_acc/tot_training_samples}\"\n",
        "      metrics += f\"\\nValidation image to text accuracy: {v_tot_image_to_text_acc/tot_validation_samples} Validation text to image accuracy: {v_tot_text_to_image_acc/tot_validation_samples}\"\n",
        "      with open(os.path.join(path, \"metrics.txt\"), \"a\") as f:\n",
        "          f.write(metrics + \"\\n\")\n",
        "      torch.save(clip_model.state_dict(), os.path.join(path, \"weights\", f\"epoch{epoch}.pt\"))\n",
        "      torch.save(optimizer.state_dict(), os.path.join(path, \"optimizer\", f\"epoch{epoch}.pt\"))\n",
        "      print(metrics)"
      ],
      "metadata": {
        "id": "gGV0P6JPIaBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save = \"\""
      ],
      "metadata": {
        "id": "bAIpb_PRhs8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(100, clip_model, clip_processor, blip_model, blip_processor, train_dataloader, val_dataloader, optimizer, path_to_save, start_epoch=0)"
      ],
      "metadata": {
        "id": "fL5r9strRLkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ofehHzEL2iD3",
        "eDSxXcqEbVmi",
        "M6SusgF3ucG9",
        "GPXR2_gyi1Zb",
        "Krm8tvOqNS8N",
        "BnqxDkmuE3u1",
        "9jympMBpFDTL",
        "byItLeJcEUoa",
        "4lYne4RDEajY",
        "Ar1pUYn-k36V"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}