{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOT\n",
    "\n",
    "This file is used to perform real life games against humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from typing import List\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method that makes the bot playable also plot the name of the card (the number). This allow the user who is controlling the bot to collect more data about the card played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot():\n",
    "    def __init__(self, blip_path_weights: str, clip_path_weights: str, images: List[Image.Image]):\n",
    "        self.images = images\n",
    "\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "        self.blip_model.load_state_dict(torch.load(blip_path_weights))\n",
    "        self.clip_model.load_state_dict(torch.load(clip_path_weights))\n",
    "\n",
    "        self.blip_model.eval()\n",
    "        self.clip_model.eval()\n",
    "\n",
    "        self.cards_in_hand = dict()\n",
    "\n",
    "    def draw_initial_hand(self, card_names: str) -> None:\n",
    "        card_names = card_names.split()\n",
    "\n",
    "        self.cards_in_hand = {name: card for name, card in self.images.items() if name in card_names}\n",
    "\n",
    "    def get_cards_in_hand(self) -> None:\n",
    "        for _, card in self.cards_in_hand.items():\n",
    "            plt.imshow(card)\n",
    "            plt.show()\n",
    "\n",
    "    def remove_card(self, card_name: str) -> None:\n",
    "        self.cards_in_hand.pop(card_name, None)\n",
    "\n",
    "    def add_card(self, card_name: str) -> None:\n",
    "        self.cards_in_hand[card_name] = self.images[card_name]\n",
    "\n",
    "    def get_caption(self) -> str:\n",
    "        name, image = random.choice([(k,v) for k,v in self.cards_in_hand.items()])\n",
    "        inputs = self.blip_processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            caption_ids = self.blip_model.generate(\n",
    "                **inputs,\n",
    "                max_length=50,  \n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,  \n",
    "                top_k=50,        \n",
    "                top_p=0.95,      \n",
    "                temperature=0.7, \n",
    "                repetition_penalty=1.2, \n",
    "                no_repeat_ngram_size=3 \n",
    "            )\n",
    "\n",
    "        caption = self.blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(caption + \"\\n\" + name)\n",
    "        plt.show()\n",
    "\n",
    "        self.remove_card(name)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    def vote_card(self, images_names_on_table: str, caption: str) -> None:\n",
    "        images_names_on_table = images_names_on_table.split()\n",
    "        images_on_table = [self.images[name] for name in images_names_on_table]\n",
    "\n",
    "        inputs = self.clip_processor(text=caption, images=images_on_table, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_model(**inputs)\n",
    "        \n",
    "        logits_per_image = outputs.logits_per_image\n",
    "\n",
    "        probs_per_image = logits_per_image.softmax(dim=0).squeeze()\n",
    "        max_score_idx = torch.argmax(probs_per_image).item()\n",
    "\n",
    "        chosen_image = images_on_table[max_score_idx]\n",
    "        image_name = images_names_on_table[max_score_idx]\n",
    "        \n",
    "        plt.imshow(chosen_image)\n",
    "        plt.title(image_name)\n",
    "        plt.show()\n",
    "\n",
    "    def choose_card(self, caption: str) -> None:\n",
    "        inputs = self.clip_processor(text=caption, images=list(self.cards_in_hand.values()), return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_model(**inputs)\n",
    "        \n",
    "        logits_per_image = outputs.logits_per_image\n",
    "\n",
    "        probs_per_image = logits_per_image.softmax(dim=0).squeeze()\n",
    "        max_score_idx = torch.argmax(probs_per_image).item()\n",
    "\n",
    "        chosen_image = list(self.cards_in_hand.values())[max_score_idx]\n",
    "        chosen_image_name = list(self.cards_in_hand.keys())[max_score_idx]\n",
    "        \n",
    "        plt.imshow(chosen_image)\n",
    "        plt.title(chosen_image_name)\n",
    "        plt.show()\n",
    "\n",
    "        self.remove_card(chosen_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards(path):\n",
    "    images = dict()\n",
    "    for image in os.listdir(path):\n",
    "        raw_image = Image.open(os.path.join(path, image)).convert(\"RGB\")\n",
    "        raw_image = raw_image.resize((224,224))\n",
    "        images[image.split(\".\")[0]] = raw_image\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "images = get_cards(\"../cards/odissey_cards\")\n",
    "\n",
    "bot = Bot(\"../weights/rephrased_blip(2nd)/epoch50.pt\", \"../weights/rephrased_coco_clip(2nd)/epoch13.pt\", images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "All the methods here accepts just the images numbers. If a list of them is required (for example when the bot has to vote the cards on the table or when it has to draw the initial hand) then the methods expect it to be a string of space separated images number.\n",
    "\n",
    "Example: bot.draw_initial_hand(\"10 7 60 42 71 78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.draw_initial_hand(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that add a card to the hand of the player\n",
    "\n",
    "Example: bot.add_card(\"79\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.add_card(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that prints the cards in the hand of the bot, to check if everything is going alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.get_cards_in_hand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that selects a random card and plot it along with the caption. It also print the caption to allow the user to copy and paste it for faster data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = bot.get_caption()\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that selects a card from the hand given a caption.\n",
    "\n",
    "Example: bot.choose_card(\"Love has no boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.choose_card(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that selects from the card given, the one that best matches the caption provided.\n",
    "\n",
    "Example: bot.vote_card(\"56 83 64\", \"Love has no boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.vote_card(\"\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
